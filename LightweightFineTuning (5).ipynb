{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef6897b",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: Lora\n",
    "* Model: GPT2\n",
    "* Evaluation approach:accuracy \n",
    "* Fine-tuning dataset: imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "404ec597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, GPT2Config,GPT2TokenizerFast, AutoModelForSequenceClassification,GPT2Tokenizer\n",
    "from transformers import GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "from collections import Counter\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import DataCollatorWithPadding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7acf416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f27c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bitsandbytes.\n",
    "#!pip install -i https://test.pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d748744",
   "metadata": {},
   "source": [
    "### Load the IMDb dataset from Hugging Face "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cfbc04",
   "metadata": {},
   "source": [
    "The IMDb dataset is a widely used benchmark dataset for binary sentiment classification tasks in natural language processing (NLP). It consists of 50,000 movie reviews taken from the Internet Movie Database (IMDb), a popular online database of information related to films, television programs, home videos, video games, and streaming content. These reviews are evenly split into two sets: 25,000 reviews for training and 25,000 for testing. Each set contains an equal number of positive and negative reviews, making it a balanced dataset.\n",
    "\n",
    "Positive reviews are those with a rating of 7 or higher out of 10, and negative reviews have a rating of 4 or lower; reviews with neutral ratings (5-6) are not included in the dataset. This clear demarcation helps in training models to distinguish between positive and negative sentiments effectively.\n",
    "\n",
    "The reviews are preprocessed and each one is encoded as plain text. Labels are provided indicating the sentiment of each review, where a label of 1 typically represents a positive sentiment and 0 represents a negative sentiment. This dataset is particularly useful for training and evaluating models on sentiment analysis tasks because it contains real-world data with a wide range of vocabulary, idiomatic expressions, and varying lengths of text, making it a comprehensive resource for training machine learning models to understand human sentiment.\n",
    "\n",
    "The IMDb dataset is not only a benchmark for sentiment analysis but also serves as a valuable resource for exploring text classification, natural language understanding, and the effectiveness of various NLP models and techniques in discerning subjective information from text.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 7.81k/7.81k [00:00<00:00, 7.02MB/s]\n",
      "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  20%|██        | 4.19M/20.5M [00:00<00:01, 15.3MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 20.5M/20.5M [00:00<00:00, 40.2MB/s]\u001b[A\n",
      "Downloading data files:  33%|███▎      | 1/3 [00:00<00:01,  1.93it/s]\n",
      "Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  20%|█▉        | 4.19M/21.0M [00:00<00:00, 34.4MB/s]\u001b[A\n",
      "Downloading data:  60%|█████▉    | 12.6M/21.0M [00:00<00:00, 52.4MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 21.0M/21.0M [00:00<00:00, 56.5MB/s]\u001b[A\n",
      "Downloading data files:  67%|██████▋   | 2/3 [00:00<00:00,  2.29it/s]\n",
      "Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  10%|▉         | 4.19M/42.0M [00:00<00:01, 30.3MB/s]\u001b[A\n",
      "Downloading data:  30%|██▉       | 12.6M/42.0M [00:00<00:00, 52.3MB/s]\u001b[A\n",
      "Downloading data:  50%|████▉     | 21.0M/42.0M [00:00<00:00, 51.0MB/s]\u001b[A\n",
      "Downloading data:  70%|██████▉   | 29.4M/42.0M [00:00<00:00, 57.5MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 42.0M/42.0M [00:00<00:00, 56.6MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.82it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 1705.23it/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 185639.27 examples/s]\n",
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 221741.58 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 239359.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Import the datasets and transformers packages\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c3fb459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoModelForCausalLM\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6982b70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 25000\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 25000\n",
       " })}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the train and test splits of the imdb dataset\n",
    "splits = [\"train\", \"test\"]\n",
    "ds = {split: ds for split, ds in zip(splits, load_dataset(\"imdb\", split=splits))}\n",
    "\n",
    "# Show the dataset\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd117b4",
   "metadata": {},
   "source": [
    "The dataset is sizable, prompting us to experiment with various split sizes to enhance accuracy. Achieving a perfect balance—50% positive and 50% negative responses—was only feasible with 2000 rows for both the training and testing datasets. However, due to GPU memory constraints, we downsized to 500 rows.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d3757ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 500\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 500\n",
       " })}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the train and test splits of the imdb dataset\n",
    "splits = [\"train\", \"test\"]\n",
    "ds = {split: ds for split, ds in zip(splits, load_dataset(\"imdb\", split=splits))}\n",
    "\n",
    "# Thin out the dataset to make it run faster for this example\n",
    "for split in splits:\n",
    "    ds[split] = ds[split].shuffle(seed=42).select(range(500))\n",
    "\n",
    "# Show the dataset\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ece328e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Label Counts: Counter({0: 254, 1: 246})\n",
      "Train Label Proportions: {1: 0.492, 0: 0.508}\n",
      "Test Label Counts: Counter({0: 254, 1: 246})\n",
      "Test Label Proportions: {1: 0.492, 0: 0.508}\n"
     ]
    }
   ],
   "source": [
    "# Dataset with 'train' and 'test' splits\n",
    "train_labels = ds['train']['label']\n",
    "test_labels = ds['test']['label']\n",
    "\n",
    "# Count the labels in both splits\n",
    "train_label_counts = Counter(train_labels)\n",
    "test_label_counts = Counter(test_labels)\n",
    "\n",
    "# Calculate proportions\n",
    "train_label_proportions = {label: count / len(train_labels) for label, count in train_label_counts.items()}\n",
    "test_label_proportions = {label: count / len(test_labels) for label, count in test_label_counts.items()}\n",
    "\n",
    "# Print the statistics\n",
    "print(\"Train Label Counts:\", train_label_counts)\n",
    "print(\"Train Label Proportions:\", train_label_proportions)\n",
    "print(\"Test Label Counts:\", test_label_counts)\n",
    "print(\"Test Label Proportions:\", test_label_proportions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "722e0591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 31936.65 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 66462.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to rename 'label' column to 'labels' for compatibility with transformer library\n",
    "def rename_label_to_labels(example):\n",
    "    example['labels'] = example['label']\n",
    "    return example\n",
    "\n",
    "# Rename the column in both the train and test datasets\n",
    "for split in ['train', 'test']:\n",
    "    ds[split] = ds[split].map(rename_label_to_labels, batched=True)\n",
    "\n",
    "# Remove the old 'label' column if you want to clean up\n",
    "for split in ['train', 'test']:\n",
    "    ds[split] = ds[split].remove_columns(['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8672f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'labels'],\n",
       "     num_rows: 500\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'labels'],\n",
       "     num_rows: 500\n",
       " })}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7036900e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few entries in train dataset:\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "First few entries in test dataset:\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exploration of the splits\n",
    "for split in splits:\n",
    "    print(f\"First few entries in {split} dataset:\")\n",
    "    for i in range(5):  # Adjust the range as needed\n",
    "        print(ds[split][i]['labels'])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1291885",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'sentiment' column for train dataset:\n",
      "[1 0]\n",
      "\n",
      "\n",
      "Unique values in 'sentiment' column for test dataset:\n",
      "[1 0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataset to be converted to a pandas DataFrame\n",
    "for split in splits:\n",
    "    df = pd.DataFrame(ds[split])\n",
    "    print(f\"Unique values in 'sentiment' column for {split} dataset:\")\n",
    "    print(df['labels'].unique())\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b79077a",
   "metadata": {},
   "source": [
    "### Preprocessing and Loading the Tokenizer from Transformer Lybrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d304d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 16.3MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 6.49MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 48.5MB/s]\n",
      "config.json: 100%|██████████| 665/665 [00:00<00:00, 2.85MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the GPT-2 tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Set the pad token to the EOS token (End Of Sentence token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21ef4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoModelForCausalLM\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6224b9",
   "metadata": {},
   "source": [
    "### Load the GPT2 Pretrained Model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63738ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 548M/548M [00:02<00:00, 222MB/s] \n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Update the GPT-2 model configuration to recognize the new padding token and the text task classification\n",
    "config = GPT2Config.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    "    pad_token_id=tokenizer.eos_token_id  # Add pad token id to the model config\n",
    ")\n",
    "\n",
    "# Load the model with the updated configuration for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", config=config)\n",
    "\n",
    "# Freeze all the parameters of the base model\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# The model is now ready with the appropriate settings for padding token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59cf7cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6ca5ee",
   "metadata": {},
   "source": [
    "### Dysplay Sample Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44eecd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 1082.29 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 1268.17 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier\\'s plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it\\'s the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...', 'labels': 1, 'input_ids': [1858, 318, 645, 8695, 379, 477, 1022, 6401, 959, 290, 4415, 5329, 475, 262, 1109, 326, 1111, 389, 1644, 2168, 546, 6590, 6741, 13, 4415, 5329, 3073, 42807, 11, 6401, 959, 3073, 6833, 13, 4415, 5329, 21528, 389, 2407, 2829, 13, 6401, 959, 338, 7110, 389, 1290, 517, 8253, 986, 6401, 959, 3073, 517, 588, 5537, 8932, 806, 11, 611, 356, 423, 284, 4136, 20594, 986, 383, 1388, 2095, 318, 4939, 290, 7650, 78, 11, 475, 423, 366, 27659, 40024, 590, 1911, 4380, 588, 284, 8996, 11, 284, 5052, 11, 284, 13446, 13, 1374, 546, 655, 13226, 30, 40473, 1517, 1165, 11, 661, 3597, 6401, 959, 3073, 1605, 475, 11, 319, 262, 584, 1021, 11, 11810, 484, 4702, 1605, 2168, 357, 10185, 737, 6674, 340, 338, 262, 3303, 11, 393, 262, 4437, 11, 475, 314, 892, 428, 2168, 318, 517, 3594, 621, 1605, 13, 2750, 262, 835, 11, 262, 10544, 389, 1107, 922, 290, 8258, 13, 383, 7205, 318, 407, 31194, 379, 477, 986, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess the dataset by returning tokenized examples with labels.\"\"\"\n",
    "    # Tokenize the text\n",
    "    tokenized_ds = tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=512)\n",
    "    #tokenized_ds = dataset.map(tokenize_function, batched=True)\n",
    "    # Add labels (ensure 'sentiment' column exists and contains appropriate labels)\n",
    "    tokenized_ds[\"labels\"] = examples[\"labels\"]\n",
    "    return tokenized_ds\n",
    "\n",
    "# Apply the preprocessing function to each split\n",
    "tokenized_ds = {split: ds[split].map(preprocess_function, batched=True) for split in splits}\n",
    "\n",
    "# Show the first example of the tokenized training set\n",
    "print(tokenized_ds[\"train\"][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ba79675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e13bad9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Sample 0:\n",
      "Input Length: 512\n",
      "Labels: 1\n",
      "\n",
      "\n",
      "TRAIN Sample 1:\n",
      "Input Length: 512\n",
      "Labels: 1\n",
      "\n",
      "\n",
      "TRAIN Sample 2:\n",
      "Input Length: 512\n",
      "Labels: 0\n",
      "\n",
      "\n",
      "TEST Sample 0:\n",
      "Input Length: 512\n",
      "Labels: 1\n",
      "\n",
      "\n",
      "TEST Sample 1:\n",
      "Input Length: 512\n",
      "Labels: 1\n",
      "\n",
      "\n",
      "TEST Sample 2:\n",
      "Input Length: 512\n",
      "Labels: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the first few samples of the training and testing datasets\n",
    "for split in ['train', 'test']:\n",
    "    for i in range(3):  # Check first 3 samples\n",
    "        print(f\"{split.upper()} Sample {i}:\")\n",
    "        print(\"Input Length:\", len(tokenized_ds[split][i]['input_ids']))\n",
    "        print(\"Labels:\", tokenized_ds[split][i]['labels'] if 'labels' in tokenized_ds[split][i] else \"No labels\")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7541da5",
   "metadata": {},
   "source": [
    "### Define Metrics to evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d46e959",
   "metadata": {},
   "source": [
    "When training a large language model (LLM) for binary text classification, such as distinguishing between positive and negative sentiments in movie reviews, relying solely on accuracy as a performance metric can be misleading. This is particularly true in scenarios where the dataset is imbalanced, meaning one class significantly outnumbers the other. In such cases, a model might achieve high accuracy by simply predicting the majority class for all instances, but this doesn't necessarily indicate that the model is performing well in identifying the nuances between classes.\n",
    "\n",
    "- **Precision and Recall**:  \n",
    "Precision (the ratio of true positive predictions to all positive predictions) and recall (the ratio of true positive predictions to all actual positives) are particularly useful when dealing with imbalanced datasets. Precision focuses on the purity of positive predictions, while recall emphasizes the completeness of positive detection.\n",
    "\n",
    "- **F1 Score**:  \n",
    "The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both. An F1 score is particularly useful when you need to balance precision and recall, and there's not a clear preference for one over the other. It is more informative than accuracy, especially in the context of imbalanced classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97088774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of the model's predictions.\n",
    "\n",
    "    This function calculates the accuracy metric for evaluating the model's performance. \n",
    "    The decision to utilize accuracy as the sole evaluation metric was influenced by constraints encountered within the Udacity workspace environment. \n",
    "    Specifically, issues importing the scikit-learn library prevented the application of the F1 score as an additional metric. \n",
    "    While the F1 score was considered and applied on a local machine, the lack of GPU resources available outside of the Udacity workspace made comprehensive evaluation impractical. \n",
    "    Consequently, to maintain project continuity and efficiency within the available infrastructure, it was decided to solely rely on accuracy for project completion.\n",
    "\n",
    "    Parameters:\n",
    "    - eval_pred: A tuple of (predictions, labels), where predictions are the logits returned by the model, and labels are the ground truth labels.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing the accuracy score under the key \"accuracy\".\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a018e9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import load_metric\n",
    "\n",
    "#accuracy_metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c760fcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the parameters of the base model\n",
    "#for param in model.base_model.parameters():\n",
    "    #param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49ffedb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\"\"\"\n",
    "This script initializes a `DataCollatorWithPadding` object from the Hugging Face Transformers library. The data collator is used to dynamically pad the inputs to the maximum length in a batch, ensuring consistent input sizes for model training or evaluation. This is particularly useful when working with variable-length sequences in NLP tasks, such as text classification, where input samples can vary significantly in length.\n",
    "\n",
    "The `DataCollatorWithPadding` takes a tokenizer as an argument, which is used to pad the inputs based on the tokenizer's padding token. This ensures that the padding is consistent with the tokenizer's specifications, allowing for seamless integration with models pre-trained using the same tokenizer.\n",
    "\n",
    "Parameters:\n",
    "- tokenizer: The tokenizer instance specific to the pre-trained model being used. This tokenizer is responsible for converting text inputs into the numerical format expected by the model, as well as applying padding.\n",
    "\n",
    "Usage:\n",
    "To use the `data_collator` with a DataLoader in PyTorch, simply pass it as the `collate_fn` argument. This ensures that each batch processed by the DataLoader is dynamically padded to the longest sequence in that batch, optimizing computational efficiency and model performance.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46872cec",
   "metadata": {},
   "source": [
    "### Testing the Model for Sentiment Analisis task classification before PEFT Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1efd1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60230826",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/sentiment_analysisft\",\n",
    "        evaluation_strategy='epoch',\n",
    "        learning_rate=2e-4,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        logging_dir='./logs',\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"].select(range(100)),  # Using a subset for faster training\n",
    "    eval_dataset=tokenized_ds[\"test\"].select(range(100)),  # Using a subset for faster evaluation\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c767e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/130 01:27, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.862696</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.647147</td>\n",
       "      <td>0.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.676570</td>\n",
       "      <td>0.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.639882</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.634058</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.635922</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.653302</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.639679</td>\n",
       "      <td>0.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.636725</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.636613</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=130, training_loss=0.7741219153771034, metrics={'train_runtime': 87.7819, 'train_samples_per_second': 11.392, 'train_steps_per_second': 1.481, 'total_flos': 261296750592000.0, 'train_loss': 0.7741219153771034, 'epoch': 10.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "#model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/sentiment_analysisft\",\n",
    "        evaluation_strategy='epoch',\n",
    "        learning_rate=2e-4,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        logging_dir='./logs',\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"].select(range(100)),  # Using a subset for faster training\n",
    "    eval_dataset=tokenized_ds[\"test\"].select(range(100)),  # Using a subset for faster evaluation\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d804541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import DataCollatorWithPadding\n",
    "\n",
    "#data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='max_length')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323e6d2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the performance of the model on the test set\n",
    "from transformers import Trainer\n",
    "# What do you think the evaluation accuracy will be?\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4237016a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configuration: GPT2Config {\n",
      "  \"_name_or_path\": \"./data/sentiment_analysis/checkpoint-25\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2ForSequenceClassification\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.32.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Tokenizer Configuration: <tokenizers.Tokenizer object at 0x2056c550>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "checkpoint = \"./data/sentiment_analysis/checkpoint-25\"#./data/sentiment_analysis/checkpoint-25#./data/sentiment_analysisft/checkpoint-75\n",
    "tokenizer \n",
    "= AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "# Print model and tokenizer details for inspection\n",
    "print(\"Model Configuration:\", model.config)\n",
    "print(\"Tokenizer Configuration:\", tokenizer.backend_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d88a6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Assuming your checkpoint is in \"path/to/your/checkpoint\"\n",
    "checkpoint = \"./data/sentiment_analysis/checkpoint-25\"\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Define dummy `TrainingArguments` for inference\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # where to save model predictions\n",
    "    do_predict=True,  # set to True for prediction\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Assuming `tokenized_ds` is your dataset prepared for the model\n",
    "#predictions = trainer.predict(tokenized_ds[\"test\"])\n",
    "\n",
    "# `predictions` now contains your model outputs\n",
    "\n",
    "# Select the first 10 examples for prediction\n",
    "subset = tokenized_ds[\"test\"].select(range(10))\n",
    "\n",
    "# Use the Trainer to predict on the selected subset\n",
    "predictions = trainer.predict(subset)\n",
    "\n",
    "# Now, `predictions` contains the outputs for the first 10 examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ec45518",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 7.437575,  8.655436],\n",
       "       [ 8.306036, 10.392733],\n",
       "       [ 9.365354,  9.832128],\n",
       "       [ 5.165733,  7.09861 ],\n",
       "       [ 9.42747 ,  9.55782 ],\n",
       "       [ 9.358664,  9.778226],\n",
       "       [ 8.487845,  9.956489],\n",
       "       [12.205111, 10.318361],\n",
       "       [ 8.701191,  7.485458],\n",
       "       [ 5.480114,  8.683409]], dtype=float32), label_ids=array([1, 1, 0, 1, 0, 1, 1, 0, 0, 1]), metrics={'test_loss': 0.33783215284347534, 'test_runtime': 1.0389, 'test_samples_per_second': 9.626, 'test_steps_per_second': 1.925})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a3ae33",
   "metadata": {},
   "source": [
    "### View Results from the model before PEFT Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5a297fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;When I unsuspectedly rented A Thou...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is the latest entry in the long series of...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie was so frustrating. Everything seem...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was truly and wonderfully surprised at \"O' B...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This movie spends most of its time preaching t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>After a very long time Marathi cinema has come...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This is a really sad, and touching movie! It d...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Don't pay any attention to the rave reviews of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Porn legend Gregory Dark directs this cheesy h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This was a great movie. Something not only for...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels  predicted_label\n",
       "0  <br /><br />When I unsuspectedly rented A Thou...       1                1\n",
       "1  This is the latest entry in the long series of...       1                1\n",
       "2  This movie was so frustrating. Everything seem...       0                1\n",
       "3  I was truly and wonderfully surprised at \"O' B...       1                1\n",
       "4  This movie spends most of its time preaching t...       0                1\n",
       "5  After a very long time Marathi cinema has come...       1                1\n",
       "6  This is a really sad, and touching movie! It d...       1                1\n",
       "7  Don't pay any attention to the rave reviews of...       0                0\n",
       "8  Porn legend Gregory Dark directs this cheesy h...       0                0\n",
       "9  This was a great movie. Something not only for...       1                1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the first 10 examples from the tokenized test dataset\n",
    "subset = tokenized_ds[\"test\"].select(range(10))\n",
    "\n",
    "# Convert the subset to a pandas DataFrame\n",
    "df = pd.DataFrame({'text': [example['text'] for example in subset], 'labels': [example['labels'] for example in subset]})\n",
    "\n",
    "# Use the trained model to make predictions on the subset\n",
    "predictions = trainer.predict(subset)\n",
    "\n",
    "# Convert raw prediction logits to discrete labels (0 or 1)\n",
    "df[\"predicted_label\"] = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Display the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b2b72e",
   "metadata": {},
   "source": [
    "## PEFT Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc65162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0eea4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00334363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18cee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configure the LoRA (Low-Rank Adaptation) parameters for fine-tuning a GPT-2 model on a sequence classification task. LoRA is designed to efficiently adapt pre-trained models with minimal additional parameters, focusing on modifying only the attention and projection layers for task-specific adjustments.\n",
    "\n",
    "Parameters:\n",
    "- r (int): Rank of the low-rank approximation. This parameter determines the size of the trainable matrices introduced by LoRA. A lower rank limits the capacity of the adaptation, while a higher rank increases it, potentially improving performance at the cost of more parameters.\n",
    "- lora_alpha (int): Alpha value for the LoRA regularization term. This controls the scaling of the low-rank matrices and can affect the extent to which LoRA modifies the pre-trained model's weights. A higher alpha increases the impact of the LoRA adjustments.\n",
    "- lora_dropout (float): Dropout rate applied to the LoRA layers. This helps prevent overfitting by randomly zeroing out some of the elements in the LoRA matrices during training.\n",
    "- bias (str): Bias type to be used in the adapted layers. Options typically include 'none', 'zero', or 'learnable'. 'None' indicates no bias, while 'learnable' allows the bias to be updated during training.\n",
    "- target_modules (list): List of module names within the model to apply LoRA to. Common targets include the attention ('c_attn') and projection ('c_proj') layers of Transformer models, as these are key to adapting the model's focus and outputs.\n",
    "- task_type (str): Type of the task for which the model is being fine-tuned. This example uses \"TaskType.SEQ_CLS\" to indicate a sequence classification task, guiding the adaptation process towards optimizing for this specific task.\n",
    "- fan_in_fan_out (bool): Whether to adjust the initialization of the low-rank matrices based on the fan-in and fan-out of the original model weights. This can help maintain the initialization scale, potentially leading to more stable training.\n",
    "\n",
    "This configuration is specifically tailored for enhancing a pre-trained GPT-2 model's performance on a text classification task using LoRA, by introducing and tuning a minimal set of additional parameters.\n",
    "\n",
    "Usage:\n",
    "Pass this configuration object to the LoRA fine-tuning process to apply these settings to the specified GPT-2 model. This approach allows for efficient, task-specific adaptation of large language models with minimal computational overhead.\n",
    "\"\"\"\n",
    "config = LoraConfig(\n",
    "    r=8,  # Rank of the low-rank approximation\n",
    "    lora_alpha=32,  # Alpha value for the LoRA regularization term\n",
    "    lora_dropout=0.01,  # Dropout rate\n",
    "    bias=\"none\",  # Bias type\n",
    "    target_modules=['c_attn', 'c_proj'],\n",
    "    task_type=\"TaskType.SEQ_CLS\",\n",
    "    fan_in_fan_out=True,  # Setting fan_in_fan_out if it's a supported parameter\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cff6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "lora_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1968daef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before setting to torch:\n",
      "Type: <class 'list'>\n",
      "Element type: <class 'int'>\n",
      "Length: 512\n",
      "\n",
      "After setting to torch:\n",
      "Type: <class 'torch.Tensor'>\n",
      "Tensor dtype: torch.int64\n",
      "Tensor shape: torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# Before setting the format to 'torch'\n",
    "print(\"Before setting to torch:\")\n",
    "print(\"Type:\", type(tokenized_ds[\"train\"][0]['input_ids']))\n",
    "# If it's a NumPy array or a list, you can also print its first element's type and length\n",
    "if isinstance(tokenized_ds[\"train\"][0]['input_ids'], (list, np.ndarray)):\n",
    "    print(\"Element type:\", type(tokenized_ds[\"train\"][0]['input_ids'][0]))\n",
    "    print(\"Length:\", len(tokenized_ds[\"train\"][0]['input_ids']))\n",
    "\n",
    "# Setting the format to 'torch'\n",
    "tokenized_ds[\"train\"].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_ds[\"test\"].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# After setting the format to 'torch'\n",
    "print(\"\\nAfter setting to torch:\")\n",
    "tensor = tokenized_ds[\"train\"][0]['input_ids']\n",
    "print(\"Type:\", type(tensor))\n",
    "print(\"Tensor dtype:\", tensor.dtype)  # Data type of the tensor\n",
    "print(\"Tensor shape:\", tensor.shape)  # Shape of the tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffb80df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Labels - Type: <class 'torch.Tensor'>\n",
      "Batch Labels - Dtype: torch.int64\n",
      "Batch Labels - Shape: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Creating a DataLoader for the training dataset\n",
    "train_dataloader = DataLoader(tokenized_ds['train'], batch_size=8)\n",
    "\n",
    "# Fetch the first batch\n",
    "first_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Inspect the 'labels' of the first batch\n",
    "if 'labels' in first_batch:\n",
    "    labels_batch = first_batch['labels']\n",
    "    print(\"Batch Labels - Type:\", type(labels_batch))\n",
    "    print(\"Batch Labels - Dtype:\", labels_batch.dtype)\n",
    "    print(\"Batch Labels - Shape:\", labels_batch.shape)  # Should be [8] for a batch size of 8\n",
    "else:\n",
    "    print(\"The batch does not contain 'labels'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2eac1d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Mask - Type: <class 'torch.Tensor'>\n",
      "Attention Mask - Dtype: torch.int64\n",
      "Attention Mask - Shape: torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# After setting the dataset format to 'torch'\n",
    "# Inspecting the 'attention_mask' tensor for the first example in the training set\n",
    "\n",
    "# Check if 'attention_mask' exists in the dataset\n",
    "if 'attention_mask' in tokenized_ds['train'].features:\n",
    "    attention_mask_tensor = tokenized_ds['train'][0]['attention_mask']\n",
    "\n",
    "    print(\"Attention Mask - Type:\", type(attention_mask_tensor))\n",
    "    print(\"Attention Mask - Dtype:\", attention_mask_tensor.dtype)\n",
    "    print(\"Attention Mask - Shape:\", attention_mask_tensor.shape)\n",
    "else:\n",
    "    print(\"The dataset does not contain an 'attention_mask' field.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b69d095",
   "metadata": {},
   "source": [
    "### Procedure for Implementing PEFT LoRA on GPT-2 for Sentiment Analysis Using Custom Training Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f16ab5",
   "metadata": {},
   "source": [
    "In the pursuit of developing a sentiment analysis model leveraging the GPT-2 architecture with Parameter-Efficient Fine-Tuning (PEFT) via Low-Rank Adaptation (LoRA), we encountered operational challenges with the Hugging Face Transformers' Trainer module. Consequently, we opted for a more hands-on approach using custom training loops within PyTorch. We could consider benefits of this approach :\n",
    "\n",
    "Customization and Control: Directly working with PyTorch provided the necessary granularity of control over every aspect of the training process, from the forward pass and loss calculation to the nuanced application of backpropagation. This level of detail was essential for integrating LoRA effectively.\n",
    "\n",
    "Optimization Strategies: The custom training approach allowed for the implementation of specialized optimization strategies tailored to the unique requirements of sentiment analysis, facilitating more effective fine-tuning of the model.\n",
    "\n",
    "Model Architecture Adjustments: It enabled precise adjustments to the model's architecture, ensuring that LoRA modifications were accurately applied and aligned with the model's sentiment analysis objectives.\n",
    "\n",
    "Despite these advantages, transitioning from a PyTorch-based training regimen to leveraging Hugging Face's Transformers library for inference presented certain drawbacks. Notably, biases in the inference results were observed, which may be attributed to the following:\n",
    "\n",
    "Model Configuration Mismatch: Potential discrepancies between the model's training configuration in PyTorch and its configuration when loaded for inference using Transformers. Such mismatches can subtly influence model performance and behavior.\n",
    "\n",
    "Tokenizer Consistency: Differences in tokenization between training and inference phases. Even minor variations in tokenization can lead to significant shifts in model output.\n",
    "\n",
    "Loss of Training Specifics: The Trainer module abstracts away many details of the training process. When moving to inference, certain nuances and optimizations of the custom training setup may not be fully captured, possibly leading to unexpected model behavior.\n",
    "\n",
    "In summary, while the direct use of PyTorch for training was necessitated by the need for deep customization and control, it also introduced probably some of observed inconsistencies when transitioning to the Transformers library for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddeadbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba46720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"eval_accuracy\": (predictions == labels).mean()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa51a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_ds[\"train\"].select(range(100)), \n",
    "                              batch_size=8, \n",
    "                              collate_fn=DataCollatorWithPadding(tokenizer=tokenizer))\n",
    "eval_dataloader = DataLoader(tokenized_ds[\"test\"].select(range(100)), \n",
    "                             batch_size=8, \n",
    "                             collate_fn=DataCollatorWithPadding(tokenizer=tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "649a20b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(lora_model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f5628a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Linear(\n",
       "            in_features=768, out_features=2304, bias=True\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (c_proj): Linear(\n",
       "            in_features=768, out_features=768, bias=True\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b470f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 13/13 [00:09<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 4.12877920957712\n",
      "Epoch 0 eval accuracy: 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:09<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 1.2221776338723989\n",
      "Epoch 1 eval accuracy: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:09<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train loss: 0.7675130848701184\n",
      "Epoch 2 eval accuracy: 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:09<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train loss: 0.5865521453894101\n",
      "Epoch 3 eval accuracy: 0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:09<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train loss: 0.49562657223298\n",
      "Epoch 4 eval accuracy: 0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:09<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train loss: 0.4158084254998427\n",
      "Epoch 5 eval accuracy: 0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:09<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train loss: 0.3991721799740425\n",
      "Epoch 6 eval accuracy: 0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:09<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train loss: 0.3293989793612407\n",
      "Epoch 7 eval accuracy: 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:09<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train loss: 0.2801224675316077\n",
      "Epoch 8 eval accuracy: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:09<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train loss: 0.25279098691848606\n",
      "Epoch 9 eval accuracy: 0.64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./datapetflora/sentiment_analysis/lora_model_GPT2/tokenizer_config.json',\n",
       " './datapetflora/sentiment_analysis/lora_model_GPT2/special_tokens_map.json',\n",
       " './datapetflora/sentiment_analysis/lora_model_GPT2/vocab.json',\n",
       " './datapetflora/sentiment_analysis/lora_model_GPT2/merges.txt',\n",
       " './datapetflora/sentiment_analysis/lora_model_GPT2/added_tokens.json',\n",
       " './datapetflora/sentiment_analysis/lora_model_GPT2/tokenizer.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "for epoch in range(10):  # Adjust the number of epochs\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch} train loss: {avg_train_loss}\")\n",
    "\n",
    "    # Evaluation Step\n",
    "    model.eval()\n",
    "    all_predictions, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    eval_accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n",
    "    # Add more metrics as needed\n",
    "    print(f\"Epoch {epoch} eval accuracy: {eval_accuracy}\")\n",
    "\n",
    "    # Saving model checkpoint at the end of each epoch\n",
    "    checkpoint_path = os.path.join('./datapetflora/sentiment_analysis/', f'checkpoint-{epoch}')\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    model.save_pretrained(checkpoint_path)\n",
    "    tokenizer.save_pretrained(checkpoint_path)\n",
    "\n",
    "# Optionally, save the final model at the end of training\n",
    "final_model_path = os.path.join('./datapetflora/sentiment_analysis/', 'lora_model_GPT2')\n",
    "os.makedirs(final_model_path, exist_ok=True)\n",
    "model.save_pretrained(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "# Optionally, save the final model at the end of training\n",
    "#if epoch == total_epochs - 1:  # Check if it's the last epoch\n",
    " #   final_model_path = os.path.join('./results/sentiment_analysis/', 'model_final.pth')\n",
    "  #  os.makedirs(os.path.dirname(final_model_path), exist_ok=True)  # Ensure the directory exists\n",
    "    \n",
    "    # Save not just the model, but also the optimizer state, and (optionally) the scheduler state\n",
    "   # torch.save({\n",
    "    #    'model_state_dict': model.state_dict(),\n",
    "     #   'optimizer_state_dict': optimizer.state_dict(),\n",
    "        # Include scheduler state if you have one\n",
    "        # 'scheduler_state_dict': scheduler.state_dict(),\n",
    "    #}, final_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a8529dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3131b3e4",
   "metadata": {},
   "source": [
    "### Quantization and Lora --QLoRa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af53aa",
   "metadata": {},
   "source": [
    "Implementing Quantization and LoRA (QLoRa) in large language models (LLMs) presents significant benefits for practical applications, particularly in enhancing the economic model of deploying AI solutions. Quantization reduces the model's memory footprint and speeds up inference by using lower-precision arithmetic, making LLMs more accessible on devices with limited computational resources. LoRA, on the other hand, allows for efficient fine-tuning of pre-trained models with minimal additional parameters, maintaining performance while reducing the computational cost of adapting models to specific tasks. Together, QLoRa can significantly lower the barriers to deploying state-of-the-art LLMs across various sectors, reducing costs and enabling real-time, on-device AI applications. This approach democratizes access to powerful AI technologies, fostering innovation and expanding the scope of AI's economic impact by making it feasible for a broader range of businesses to integrate advanced natural language processing capabilities into their operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcfc41c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdfa688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a157db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of the low-rank approximation\n",
    "    lora_alpha=32,  # Alpha value for the LoRA regularization term\n",
    "    lora_dropout=0.01,  # Dropout rate\n",
    "    bias=\"none\",  # Bias type\n",
    "    target_modules=['c_attn', 'c_proj'],\n",
    "    task_type=\"TaskType.SEQ_CLS\",\n",
    "    fan_in_fan_out=True,  # Setting fan_in_fan_out if it's a supported parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12ed9851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "q_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    q_4bit_quant_type=\"nf4\",\n",
    "    q_4bit_use_double_quant=True,\n",
    "    q_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4c31796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Update the GPT-2 model configuration to recognize the new padding token and the text task classification\n",
    "config = GPT2Config.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    "    pad_token_id=tokenizer.eos_token_id  # Add pad token id to the model config\n",
    ")\n",
    "\n",
    "# Load the model with the updated configuration for sequence classification\n",
    "q_model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", config=config,quantization_config=q_config)\n",
    "\n",
    "\n",
    "# Freeze all the parameters of the base model\n",
    "for param in q_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# The model is now ready with the appropriate settings for padding token\n",
    "q_model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "lora_model = get_peft_model(q_model, lora_config,\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c154017a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 500\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 500\n",
       " })}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1121f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds[\"train\"].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_ds[\"test\"].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ceab042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4d93309",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_ds[\"train\"].select(range(100)), \n",
    "                              batch_size=8, \n",
    "                              collate_fn=DataCollatorWithPadding(tokenizer=tokenizer))\n",
    "eval_dataloader = DataLoader(tokenized_ds[\"test\"].select(range(100)), \n",
    "                             batch_size=8, \n",
    "                             collate_fn=DataCollatorWithPadding(tokenizer=tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba559b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(lora_model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe6f5cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:224: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n",
      "100%|██████████| 13/13 [00:07<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 4.716947115384615\n",
      "Epoch 0 eval accuracy: 0.5480769276618958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:07<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 1.33349609375\n",
      "Epoch 1 eval accuracy: 0.5384615659713745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:07<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train loss: 0.7529860276442307\n",
      "Epoch 2 eval accuracy: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:07<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train loss: 0.6777719350961539\n",
      "Epoch 3 eval accuracy: 0.6153846383094788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:07<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train loss: 0.6040602463942307\n",
      "Epoch 4 eval accuracy: 0.6153846383094788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:07<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train loss: 0.5526968149038461\n",
      "Epoch 5 eval accuracy: 0.6153846383094788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:07<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train loss: 0.4857741135817308\n",
      "Epoch 6 eval accuracy: 0.634615421295166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:07<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train loss: 0.44410118689903844\n",
      "Epoch 7 eval accuracy: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:07<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train loss: 0.3713425856370192\n",
      "Epoch 8 eval accuracy: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:07<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train loss: 0.3341815655048077\n",
      "Epoch 9 eval accuracy: 0.634615421295166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:07<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 train loss: 0.2551069993239183\n",
      "Epoch 10 eval accuracy: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:07<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 train loss: 0.1791053185096154\n",
      "Epoch 11 eval accuracy: 0.6442307829856873\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lora_model.to(device)\n",
    "\n",
    "for epoch in range(12):  # number of epochs\n",
    "    lora_model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = lora_model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch} train loss: {train_loss / len(train_dataloader)}\")\n",
    "\n",
    "    # Evaluation\n",
    "    lora_model.eval()\n",
    "    eval_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = lora_model(**batch)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            eval_accuracy += (predictions == batch[\"labels\"]).float().mean()\n",
    "\n",
    "    eval_accuracy = eval_accuracy / len(eval_dataloader)\n",
    "    print(f\"Epoch {epoch} eval accuracy: {eval_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b47abf88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 811,008 || all params: 125,252,352 || trainable%: 0.6474992182182735\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6bae0f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lora_model.save_pretrained(\"gpt-qlora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8794759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2ForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8ba3fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "# Correct checkpoint path\n",
    "checkpoint_path = './data/sentiment_analysis/checkpoint-25'\n",
    "\n",
    "# Load the model and assign it to a variable\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Assuming 'tokenized_ds' and 'tokenizer' have been correctly defined earlier\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        # Specify a directory for saving training outputs\n",
    "        output_dir='./data/sentiment_analysis/training_output',\n",
    "        # Correct the lorategy='epoch',\n",
    "        learning_rate=2e-4,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir='./data/sentiment_analysis/logs',\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"].select(range(100)),  # Using a subset for faster training\n",
    "    eval_dataset=tokenized_ds[\"test\"].select(range(100)),  # Using a subset for faster evaluation\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7f9f5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 05:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6418538689613342,\n",
       " 'eval_accuracy': 0.63,\n",
       " 'eval_runtime': 360.6548,\n",
       " 'eval_samples_per_second': 0.277,\n",
       " 'eval_steps_per_second': 0.036}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the performance of the model on the test set\n",
    "from transformers import Trainer\n",
    "# What do you think the evaluation accuracy will be?\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58b95c",
   "metadata": {},
   "source": [
    "## Lora Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0919bf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./datapetflora/sentiment_analysis/checkpoint-9 were not used when initializing GPT2ForSequenceClassification: ['transformer.h.2.attn.c_proj.lora_A.default.weight', 'transformer.h.6.attn.c_proj.lora_B.default.weight', 'transformer.h.2.attn.c_attn.lora_B.default.weight', 'transformer.h.4.mlp.c_proj.lora_A.default.weight', 'transformer.h.11.mlp.c_proj.lora_A.default.weight', 'transformer.h.6.mlp.c_proj.lora_A.default.weight', 'transformer.h.2.mlp.c_proj.lora_A.default.weight', 'transformer.h.7.attn.c_attn.lora_A.default.weight', 'transformer.h.9.mlp.c_proj.lora_B.default.weight', 'transformer.h.7.attn.c_proj.lora_A.default.weight', 'transformer.h.1.attn.c_proj.lora_A.default.weight', 'transformer.h.10.mlp.c_proj.lora_A.default.weight', 'transformer.h.6.mlp.c_proj.lora_B.default.weight', 'transformer.h.0.mlp.c_proj.lora_A.default.weight', 'transformer.h.11.mlp.c_proj.lora_B.default.weight', 'transformer.h.8.attn.c_proj.lora_A.default.weight', 'transformer.h.8.mlp.c_proj.lora_A.default.weight', 'transformer.h.5.attn.c_proj.lora_B.default.weight', 'transformer.h.5.mlp.c_proj.lora_B.default.weight', 'transformer.h.1.attn.c_proj.lora_B.default.weight', 'transformer.h.5.attn.c_attn.lora_B.default.weight', 'transformer.h.5.attn.c_attn.lora_A.default.weight', 'transformer.h.8.attn.c_attn.lora_A.default.weight', 'transformer.h.1.attn.c_attn.lora_A.default.weight', 'transformer.h.4.attn.c_proj.lora_A.default.weight', 'transformer.h.10.attn.c_attn.lora_A.default.weight', 'transformer.h.7.mlp.c_proj.lora_A.default.weight', 'transformer.h.0.attn.c_attn.lora_A.default.weight', 'transformer.h.4.attn.c_attn.lora_A.default.weight', 'transformer.h.3.attn.c_proj.lora_A.default.weight', 'transformer.h.6.attn.c_attn.lora_B.default.weight', 'transformer.h.3.attn.c_attn.lora_A.default.weight', 'transformer.h.11.attn.c_attn.lora_B.default.weight', 'transformer.h.9.attn.c_attn.lora_B.default.weight', 'transformer.h.4.attn.c_proj.lora_B.default.weight', 'transformer.h.6.attn.c_proj.lora_A.default.weight', 'transformer.h.4.mlp.c_proj.lora_B.default.weight', 'transformer.h.7.attn.c_attn.lora_B.default.weight', 'transformer.h.9.attn.c_attn.lora_A.default.weight', 'transformer.h.7.attn.c_proj.lora_B.default.weight', 'transformer.h.4.attn.c_attn.lora_B.default.weight', 'transformer.h.1.attn.c_attn.lora_B.default.weight', 'transformer.h.3.mlp.c_proj.lora_B.default.weight', 'transformer.h.8.attn.c_attn.lora_B.default.weight', 'transformer.h.0.attn.c_proj.lora_B.default.weight', 'transformer.h.9.attn.c_proj.lora_B.default.weight', 'transformer.h.3.attn.c_attn.lora_B.default.weight', 'transformer.h.3.mlp.c_proj.lora_A.default.weight', 'transformer.h.10.attn.c_attn.lora_B.default.weight', 'transformer.h.2.mlp.c_proj.lora_B.default.weight', 'transformer.h.5.attn.c_proj.lora_A.default.weight', 'transformer.h.7.mlp.c_proj.lora_B.default.weight', 'transformer.h.6.attn.c_attn.lora_A.default.weight', 'transformer.h.0.attn.c_proj.lora_A.default.weight', 'transformer.h.1.mlp.c_proj.lora_A.default.weight', 'transformer.h.10.attn.c_proj.lora_A.default.weight', 'transformer.h.5.mlp.c_proj.lora_A.default.weight', 'transformer.h.8.attn.c_proj.lora_B.default.weight', 'transformer.h.9.attn.c_proj.lora_A.default.weight', 'transformer.h.0.attn.c_attn.lora_B.default.weight', 'transformer.h.11.attn.c_attn.lora_A.default.weight', 'transformer.h.10.mlp.c_proj.lora_B.default.weight', 'transformer.h.10.attn.c_proj.lora_B.default.weight', 'transformer.h.2.attn.c_proj.lora_B.default.weight', 'transformer.h.11.attn.c_proj.lora_B.default.weight', 'transformer.h.1.mlp.c_proj.lora_B.default.weight', 'transformer.h.11.attn.c_proj.lora_A.default.weight', 'transformer.h.8.mlp.c_proj.lora_B.default.weight', 'transformer.h.0.mlp.c_proj.lora_B.default.weight', 'transformer.h.3.attn.c_proj.lora_B.default.weight', 'transformer.h.9.mlp.c_proj.lora_A.default.weight', 'transformer.h.2.attn.c_attn.lora_A.default.weight']\n",
      "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, GPT2ForSequenceClassification\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "# Load the model from the correct checkpoint path\n",
    "#checkpoint_path = r\"C:\\Users\\Atilio\\anaconda3\\envs\\envs\\LangChain\\LLMAppsUdemy\\gpt-lora\\\"\n",
    "checkpoint_path = './datapetflora/sentiment_analysis/checkpoint-9'\n",
    "\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "# Load the tokenizer from the checkpoint\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(checkpoint_path)\n",
    "#model.load_adapter(\"gpt-lora\")\n",
    "\n",
    "# Assuming 'tokenized_ds' and 'tokenizer' have been correctly defined earlier\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir='./datapeftlora/sentiment_analysis/training_output',\n",
    "        evaluation_strategy='epoch',\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        learning_rate=2e-4,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        logging_dir= './logs',\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"].select(range(100)),  # Replace with your full training dataset\n",
    "    eval_dataset=tokenized_ds[\"test\"].select(range(100)),   # Replace with your full evaluation dataset\n",
    "    tokenizer=tokenizer,  # Make sure this is correctly initialized\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer, padding=True),\n",
    "    compute_metrics=compute_metrics,# Complete the data collator definition\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30a52c97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 04:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 5.193200588226318,\n",
       " 'eval_accuracy': 0.47,\n",
       " 'eval_runtime': 305.5018,\n",
       " 'eval_samples_per_second': 0.327,\n",
       " 'eval_steps_per_second': 0.043}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the performance of the model on the test set\n",
    "from transformers import Trainer\n",
    "# What do you think the evaluation accuracy will be?\n",
    "trainer.evaluate()# What do you think the evaluation accuracy will be?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c205fad",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b72201f",
   "metadata": {},
   "source": [
    "In refining a sentiment analysis model with GPT-2 using LoRA and QLoRA, we faced challenges that limited our training scalability due to CUDA memory constraints, on the other hand we have never obtain during the training process better accuracy than 0.73 despite a consistent decrease in training loss. The issue appears to stem from improper model-saving practices, potentially saving early training states rather than optimized final states, suggesting a misalignment in model serialization between PyTorch and Transformers frameworks. This situation underscores the importance of precise model management and addressing hardware limitations to harness the full potential of advanced fine-tuning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897fe1db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
